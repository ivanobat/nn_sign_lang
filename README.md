# Sign language detection using neural networks
American Sign Language (ASL) is a natural language that serves as the predominant sign language of Deaf communities in the United States and most of Anglophone Canada. ASL is a complete and organized visual language that is expressed by facial expression as well as movements and motions with the hands. In this project we will focus on two approaches to the ASL detection: 
1. In the first part, we will follow the YOLO proposed project with all its requirements, which includes reading reference papers, applying YOLO to a custom dataset and discussing results.
2. In the second part we demonstrate how to build from scratch a couple of basic neural networks and compare basic architectures, differences and results for each of them.

We believe that understanding YOLO and basic architectures fully will provide the most coverage in the learning of the inner workings of convolutional neural networks and will allow us to sketch a timeline on the high speed of progress in this area of deep learning.

All the code relevant to complete this exercise was completed in python and is available in two notebooks, corresponding to each part of the project.

Dataset preview

![image](https://github.com/user-attachments/assets/805444a9-216b-47c0-8ee2-3de5a36a3739)

CNN architecture used

![image](https://github.com/user-attachments/assets/f72595b7-283f-4489-a399-a44abd03b21a)

Test accuracy between simple and CNN approach

![image](https://github.com/user-attachments/assets/01bc4fc7-459d-4ca1-ae5b-c035518d9d13)


# Dataset
28 pixels images https://www.kaggle.com/datamunge/sign-language-mnist

64 pixels images https://www.kaggle.com/grassknoted/asl-alphabet
